{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse #renamed to urlib.parse in python 3\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import as_completed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for scraping listings from Airbnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDriver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\") # runs chrome without actually opening the chrome window\n",
    "    # options.add_argument(\"--incognito\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "def initSoup(driver):\n",
    "    html_content = driver.page_source\n",
    "    return BeautifulSoup(html_content,\"html.parser\")\n",
    "\n",
    "def getRootUrl(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    root_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "    return root_url\n",
    "\n",
    "def waitForListingElements(driver):\n",
    "    # listing container\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.XPATH, \"//div[@itemprop='itemListElement']\")))\n",
    "    \n",
    "    # pagination container\n",
    "    element = WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.CLASS_NAME, \"p1j2gy66\")))\n",
    "    \n",
    "\n",
    "def scrapeListings(driver, soup, root_url): # 1 page\n",
    "    listings = soup.find_all(\"div\",{\"itemprop\":\"itemListElement\"})\n",
    "    page_data = pd.DataFrame(columns=[\"Place type\", \"Room type\",\"Location\", \"Rating\", \"Total Reviews\", \"Price/night (SGD)\", \"Total Price (SGD)\", \"Link\"])\n",
    "    for listing in listings:\n",
    "        try:\n",
    "            title = listing.find(\"div\",{\"data-testid\":\"listing-card-title\"}).text.strip().split(\"in\")\n",
    "            room_type = title[0].strip()\n",
    "            if room_type.lower() == \"room\":\n",
    "                place_type = \"Room\"\n",
    "            else:\n",
    "                place_type = \"Entire home\"\n",
    "            location = title[1].strip()\n",
    "            rating = listing.find(\"div\",{\"class\":\"t1a9j9y7\"}).text.strip().split()[0]\n",
    "            if rating != \"New\":\n",
    "                totalReviews = listing.find(\"div\",{\"class\":\"t1a9j9y7\"}).text.strip().split()[6]\n",
    "            else:\n",
    "                totalReviews = 0\n",
    "            price = listing.find(\"span\",{\"class\":\"_11jcbg2\"}).text.strip().split()[0]\n",
    "            price_tax = listing.find(\"div\",{\"class\":\"_i5duul\"}).find(\"div\",{\"class\":\"_10d7v0r\"}).text.strip().split()[0]\n",
    "            link = listing.find(\"a\", {\"class\":\"l1ovpqvx\"}).get(\"href\")\n",
    "            # Reconstruct the absolute link\n",
    "            link = root_url+link\n",
    "            current_data = pd.DataFrame({\n",
    "                \"Place type\":[place_type],\n",
    "                \"Room type\": [room_type],\n",
    "                \"Location\": [location],\n",
    "                \"Rating\":[rating],\n",
    "                \"Total Reviews\":[totalReviews],\n",
    "                \"Price/night (SGD)\":[price],\n",
    "                \"Total Price (SGD)\":[price_tax],\n",
    "                \"Link\":[link]\n",
    "            })\n",
    "            page_data = pd.concat([page_data, current_data], axis=0)\n",
    "            # print(f\"Title: {title}\\nRating: {rating}/5\\nPrice: {price}\\nTotal Price: {price_tax}\\nLink: {link} \\n\\n\")\n",
    "        \n",
    "        # skip the current listing if contain missing info/error\n",
    "        except:\n",
    "            continue\n",
    "    return page_data\n",
    "\n",
    "def getnextPageURL(soup, root_url):\n",
    "    next_link = soup.find(\"a\",{\"aria-label\":\"Next\"})\n",
    "    # check for last page\n",
    "    if next_link:\n",
    "        next_page = root_url+next_link.get(\"href\")\n",
    "        return next_page\n",
    "    else:\n",
    "        return next_link # will return false\n",
    "\n",
    "def quitProgram(driver, airbnb_data, path):\n",
    "    driver.quit()\n",
    "    file_path = Path(path)\n",
    "    file_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "    airbnb_data.to_csv(file_path, index = False)\n",
    "    print(f\"Data saved to {file_path}...\")\n",
    "\n",
    "'''====================================================================='''\n",
    "\n",
    "def scrapeListingDetails(url):\n",
    "    driver = initDriver()\n",
    "    listing_details = {}\n",
    "    driver.get(url)\n",
    "    # close the pop-up dialog for translation\n",
    "    try:\n",
    "        modal = WebDriverWait(driver, 5).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@role='dialog']\")))\n",
    "        close_button = modal.find_element(By.XPATH, \"//button[@aria-label='Close']\")\n",
    "        close_button.click()\n",
    "    except TimeoutException:\n",
    "        pass\n",
    "    except NoSuchElementException:\n",
    "        print(\"Couldn't find translation modal\") \n",
    "    try:\n",
    "        # 1\n",
    "        soup = initSoup(driver)\n",
    "        listing_details.update(scrapeRoomContents(soup))\n",
    "\n",
    "        # 2\n",
    "        soup = initSoup(driver)\n",
    "        listing_details.update(scrapeLatLong(soup))   \n",
    "\n",
    "        # 3 - only need pass the url in 1 of the dictionary for merging with the original dataframe\n",
    "        # click the show all amenitites button\n",
    "        amentities_sect = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@data-section-id='AMENITIES_DEFAULT']\")))\n",
    "        amenities_button = WebDriverWait(amentities_sect, 10).until(\n",
    "        EC.element_to_be_clickable((By.TAG_NAME, \"button\")))\n",
    "        amenities_button.click()\n",
    "        amenities_modal = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.XPATH, \"//div[@role='dialog']\")))\n",
    "        # current_url = driver.current_url\n",
    "        soup = initSoup(driver)\n",
    "        listing_details.update(scrapeAmenities(soup, url))\n",
    "\n",
    "        # 4\n",
    "        soup = initSoup(driver)\n",
    "        listing_details.update(scrapeRatings(soup))\n",
    "\n",
    "        # 5 \n",
    "        soup = initSoup(driver)\n",
    "        listing_details.update(scrapeHostDetails(soup))\n",
    "    # skip the current listing, scrape again later\n",
    "    # except Exception as e:\n",
    "    #     driver.quit()\n",
    "    #     listing_details = {\"Link\":url}\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return pd.DataFrame([listing_details])\n",
    "\n",
    "def scrapeRoomContents(soup):\n",
    "    room_contents = soup.find(\"div\", {\"class\":\"o1kjrihn\"}).find_all(\"li\", {\"class\":\"l7n4lsf\"})\n",
    "    bedrooms, beds, bathrooms = 0, 0, 0\n",
    "    for content in room_contents:\n",
    "        content = content.text.strip(\"Â· \")\n",
    "        if 'bedroom' in content:\n",
    "            bedrooms = content.split(' ')[0]\n",
    "        elif 'beds' in content:\n",
    "            beds = content.split(' ')[0]\n",
    "        elif 'bathroom' in content:\n",
    "            bathrooms= content.split(' ')[0]\n",
    "            try: # set bathrooms to 1 regardless of the desc like private etc.\n",
    "                int_value = int(bathrooms)\n",
    "            except ValueError:\n",
    "                bathrooms = 1\n",
    "        airbnb_room_contents = {\n",
    "            \"Bedrooms\":bedrooms,\n",
    "            \"Beds\":beds, \n",
    "            \"Bathrooms\":bathrooms,\n",
    "        }\n",
    "\n",
    "    return airbnb_room_contents\n",
    "\n",
    "def scrapeRatings(soup):\n",
    "    ratings = soup.find_all(\"div\", {\"class\":\"l925rvg\"})\n",
    "    cleanliness, accuracy, checkIn, communication, location, value = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "    for rating in ratings:\n",
    "        name = rating.contents[0].text\n",
    "        number = rating.contents[1].text\n",
    "        if name == \"Cleanliness\":\n",
    "            cleanliness = number\n",
    "        elif name == \"Accuracy\":\n",
    "            accuracy = number\n",
    "        elif name == \"Check-in\":\n",
    "            checkIn = number\n",
    "        elif name == \"Communication\":\n",
    "            communication = number\n",
    "        elif name == \"Location\":\n",
    "            location = number\n",
    "        elif name == \"Value\":\n",
    "            value = number\n",
    "    airbnb_ratings = {\n",
    "        \"Cleanliness Rating\":cleanliness,\n",
    "        \"Accuracy Rating\":accuracy,\n",
    "        \"Check-in Rating\":checkIn,\n",
    "        \"Communication Rating\":communication,\n",
    "        \"Location Rating\":location,\n",
    "        \"Value Rating\":value,\n",
    "    }\n",
    "    \n",
    "    return airbnb_ratings\n",
    "\n",
    "def scrapeAmenities(soup, url):\n",
    "    amenities = soup.find_all(\"div\", {\"class\":\"twad414\"})\n",
    "    airbnb_amenities = []\n",
    "    for amenity in amenities:\n",
    "        if (amenity.text.startswith(\"Unavailable\")):\n",
    "            continue\n",
    "        airbnb_amenities.append(amenity.text)\n",
    "    airbnb_amenities = {\n",
    "        \"Amenities\":airbnb_amenities,\n",
    "        \"Link\":url\n",
    "    }\n",
    "\n",
    "    return airbnb_amenities\n",
    "\n",
    "def scrapeHostDetails(soup):\n",
    "    host_superhost = soup.find(\"span\", {\"class\":\"s2nv573\"})\n",
    "    if host_superhost and host_superhost.text == \"Superhost\":\n",
    "        host_superhost = True\n",
    "    else:\n",
    "        host_superhost = False\n",
    "    host_reviews = soup.find(\"span\", {\"data-testid\":\"Reviews-stat-heading\"})\n",
    "    host_rating = soup.find(\"div\", {\"class\":\"ruujrrq\"})\n",
    "    host_reviews = host_reviews.text if host_reviews else float(np.nan)\n",
    "    host_rating = host_rating.text if host_rating else float(np.nan)\n",
    "\n",
    "    airbnb_host_details = {\n",
    "        \"Superhost\":host_superhost,\n",
    "        \"Host Reviews\":host_reviews,\n",
    "        \"Host Rating\":host_rating,\n",
    "    }\n",
    "\n",
    "    return airbnb_host_details\n",
    "\n",
    "def scrapeLatLong(soup):\n",
    "    script = soup.find(\"script\", string=lambda x: x and 'lat' in x and 'lng' in x)\n",
    "    if script:\n",
    "        match = re.search(r'\"lat\":([\\d.]+),\"lng\":([\\d.]+)', script.text)\n",
    "        if match:\n",
    "            latitude = match.group(1)\n",
    "            longitude = match.group(2)\n",
    "    airbnb_lat_long =  {\n",
    "        \"Latitude\":latitude,\n",
    "        \"Longitude\":longitude,\n",
    "    }\n",
    "\n",
    "    return airbnb_lat_long  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape basic information of listings from page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to dataset\\airbnb.csv...\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.airbnb.com.sg/s/Bangkok--Thailand/homes?refinement_paths%5B%5D=%2Fhomes&checkin=2024-10-04&checkout=2024-10-10&adults=3&tab_id=home_tab&query=Bangkok%2C%20Thailand&flexible_trip_lengths%5B%5D=one_week&monthly_start_date=2024-08-01&monthly_length=3&monthly_end_date=2024-11-01&price_filter_input_type=0&price_filter_num_nights=6&channel=EXPLORE&date_picker_type=calendar&place_id=ChIJ82ENKDJgHTERIEjiXbIAAQE&source=structured_search_input_header&search_type=user_map_move&search_mode=regular_search&ne_lat=13.810393924485789&ne_lng=100.5597668742775&sw_lat=13.71428581582692&sw_lng=100.48846329668004&zoom=12.786708747368644&zoom_level=12.786708747368644&search_by_map=true\"\n",
    "root_url = getRootUrl(url)\n",
    "driver = initDriver()\n",
    "\n",
    "airbnb_data = pd.DataFrame(columns=[\"Place type\", \"Room type\", \"Location\", \"Rating\", \"Total Reviews\", \"Price/night (SGD)\", \"Total Price (SGD)\", \"Link\"])\n",
    "\n",
    "try:\n",
    "    driver.get(url)\n",
    "    while True:\n",
    "        waitForListingElements(driver)\n",
    "        soup = initSoup(driver)\n",
    "        data = scrapeListings(driver, soup, root_url)\n",
    "        airbnb_data = pd.concat([airbnb_data, data], axis = 0)\n",
    "        result = getnextPageURL(soup, root_url)\n",
    "        if(result):\n",
    "            driver.get(result)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "\n",
    "finally:\n",
    "    path = \"./dataset/airbnb.csv\"\n",
    "    quitProgram(driver, airbnb_data, path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape additional information from each listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "Amenities has been successfully merged into data\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/airbnb.csv\", header=0)\n",
    "listing_url = data[\"Link\"].to_list()\n",
    "count = 0\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(scrapeListingDetails, url) for url in listing_url]\n",
    "    for future in as_completed(futures):\n",
    "        results.append(future.result())\n",
    "        count += 1\n",
    "        print(count)\n",
    "final_data = pd.concat(results, axis=0)\n",
    "data = data.merge(final_data, how=\"inner\", on=\"Link\")\n",
    "data = data.reindex(columns = [\"Place type\", \"Room type\", \"Location\", \"Latitude\", \"Longitude\",\n",
    "    \"Price/night (SGD)\", \"Total Price (SGD)\", \"Bedrooms\", \"Beds\", \"Bathrooms\",\n",
    "    \"Superhost\", \"Host Reviews\", \"Host Rating\", \"Total Reviews\", \"Rating\",\n",
    "    \"Cleanliness Rating\", \"Accuracy Rating\", \"Check-in Rating\",\n",
    "    \"Communication Rating\", \"Location Rating\", \"Value Rating\", \"Amenities\", \"Link\"])\n",
    "file_path = Path(\"./dataset/airbnb.csv\")\n",
    "file_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "data.to_csv(file_path, index = False)\n",
    "print(\"Amenities has been successfully merged into data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.736341903712638 100.55670793589738\n"
     ]
    }
   ],
   "source": [
    "import requests, re\n",
    "\n",
    "r = requests.get('https://www.airbnb.com.sg/rooms/1195697419656803553?check_out=2024-10-10&unique_share_id=F5DB2A4D-DA11-4439-AA2E-687AB8180DD5&slcid=f1247430419743e5bc6306bd76a620a0&s=13&feature=share&adults=3&check_in=2024-10-04&channel=whatsapp&slug=9O4FjOpF&source_impression_id=p3_1721802607_P3n6jNcn1igcjEQg')\n",
    "p_lat = re.compile(r'\"lat\":([-0-9.]+),')\n",
    "p_lng = re.compile(r'\"lng\":([-0-9.]+),')\n",
    "lat = p_lat.findall(r.text)[0]\n",
    "lng = p_lng.findall(r.text)[0]\n",
    "print(lat,lng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this cell only if there are missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Place type              0\n",
      "Room type               0\n",
      "Location                0\n",
      "Latitude                0\n",
      "Longitude               0\n",
      "Price/night (SGD)       0\n",
      "Total Price (SGD)       0\n",
      "Bedrooms                0\n",
      "Beds                    0\n",
      "Bathrooms               0\n",
      "Superhost               0\n",
      "Host Reviews            4\n",
      "Host Rating             4\n",
      "Total Reviews           0\n",
      "Rating                  0\n",
      "Cleanliness Rating      0\n",
      "Accuracy Rating         0\n",
      "Check-in Rating         0\n",
      "Communication Rating    0\n",
      "Location Rating         0\n",
      "Value Rating            0\n",
      "Amenities               0\n",
      "Link                    0\n",
      "dtype: int64\n",
      "136    https://www.airbnb.com.sg/rooms/28263438?adult...\n",
      "168    https://www.airbnb.com.sg/rooms/12172154329531...\n",
      "234    https://www.airbnb.com.sg/rooms/2960283?adults...\n",
      "270    https://www.airbnb.com.sg/rooms/11860043030382...\n",
      "Name: Link, dtype: object\n",
      "Place type              0\n",
      "Room type               0\n",
      "Location                0\n",
      "Latitude                0\n",
      "Longitude               0\n",
      "Price/night (SGD)       0\n",
      "Total Price (SGD)       0\n",
      "Bedrooms                0\n",
      "Beds                    0\n",
      "Bathrooms               0\n",
      "Superhost               0\n",
      "Host Reviews            0\n",
      "Host Rating             0\n",
      "Total Reviews           0\n",
      "Rating                  0\n",
      "Cleanliness Rating      0\n",
      "Accuracy Rating         0\n",
      "Check-in Rating         0\n",
      "Communication Rating    0\n",
      "Location Rating         0\n",
      "Value Rating            0\n",
      "Amenities               0\n",
      "Link                    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"./dataset/airbnb.csv\", header=0)\n",
    "print(data.isna().sum())\n",
    "null_mask = data.isna().any(axis=1)\n",
    "# retrieve the URLs with no amenities\n",
    "null_url = data[null_mask][\"Link\"]\n",
    "print(null_url)\n",
    "\n",
    "data = data.dropna()\n",
    "print(data.isna().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
